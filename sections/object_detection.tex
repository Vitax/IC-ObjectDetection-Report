\begin{document}
\section{Introduction to Neurons and Networks}
Similar to the neurons in the human brain, neurons in computer science are approaching the same concept and same ideology.\\
The way a neuron functions is to retrieve data, possibly and ideally multiple at the same time, process and output a single result. There
are many types of neurons, for the purpose of this report I am just gonna explain two of them:
\begin{itemize}
    \item Perceptrons
    \item Sigmoids
\end{itemize}
The more commonly used artificial neuron in Machine Learning is the Sigmoid. The reasons for that will be mentioned in the chapters
below.

\subsection{Percpetrons}
Perceptrons were developed in 1950 to 1960 by the scientist Frank Rosenblatt, inspired by earlier works from Waren McCulloch and Walter
Pitts.\cite{neuralnetworksanddeeplearning}
\begin{center}
    \includegraphics[width=0.25\textwidth]{images/neurons/simple_neuron.png}
\end{center}
In this example our neuron has 2 input variables and one output, in theory it can have more or less. Further more Frank Rosenblatt proposes
the concept of weights for each input variable defining how important a variable is compared to another.\\
\begin{equation*}
    output=
    \begin{cases}
        0 \text{ if } \sum{_j}{w_jx_j \le threshold}\\
        1 \text{ if } \sum{_j}{w_jx_j > threshold}
    \end{cases}
\end{equation*}
\newpage \noindent
A single neuron is by no means comparable to a decision making of a human being, but it can weigh variables and make decisions
accordingly by defining a threshold for when a true or false should be evaluated. Another way to create a conditional behavior is to use a bias
instead of a threshold.
\begin{center}
    \includegraphics[width=0.25\textwidth]{images/neurons/simple_neuron_bias.png}
\end{center}
\begin{equation*}
    output=
    \begin{cases}
        0 \text{ if } \sum{_j}{w_jx_j+b \le 0}\\
        1 \text{ if } \sum{_j}{w_jx_j+b > 0}
    \end{cases}
\end{equation*}
A whole network of these neurons is able to perform more complex decision making. The network below is a feed forward network, but there are
bidirectional and even forms where the neuron passes its output back to itself as an input.
\begin{center}
    \includegraphics[width=0.5\textwidth]{images/neurons/neural_network.png}
\end{center}
\newpage \noindent
The columns in this network are called layers, the first column is refered to as the first layers and the last one as the output one all the
ones in between are called hidden layers. \\
Since the result of a perceptron can only be one or zero (true or false), one major problem someone faces when using perceptrons in more
complex neural networks is that biases and weights can have weird and unwanted effects. Making perceptrons trigger where they should not.
Thats why sigmoids were invented and they build on top of the concept of perceptrons.

\subsection{Sigmoids}
The core principle of a sigmoid neuron is the same as the perceptrons. Just like perceptrons, sigmoids have input and output variables but
instead of just being 0 or 1 it can be anything in between 0 and 1 so 0.5234 is a valid output of a sigmoid. Likewise it can have weights and
biases functioning the same as it does for the perceptrons.
The core difference is its arithmetic function, called the sigmoid function or sometimes the logistic function.
\begin{equation*}
    \begin{split}
        \sigma(z) & = \frac{1}{1+e^{-z}}
    \end{split}
\end{equation*}
To put it more explicit with input variables, weights and bias:
\begin{equation*}
    \begin{split}
        \frac{1}{1+exp(-\sum{_j}{w_jx_j-b})}
    \end{split}
\end{equation*}
\begin{center}
    \begin{tikzpicture}
        \begin{axis}[
                width=0.5\textwidth,
                height=0.25\textwidth,
                axis x line=bottom,
                xmin=-4,
                xmax=4,
                xlabel near ticks,
                axis y line=left,
                ymin=0,
                ymax=1,
                ylabel near ticks
            ]
            \addplot[color=red, domain=-4:4, width=2] (x,{1/(1+exp(-x))});
        \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}
        \begin{axis}[
                width=0.5\textwidth,
                height=0.25\textwidth,
                axis x line=bottom,
                xmin=-4,
                xmax=4,
                xlabel near ticks,
                axis y line=left,
                ymin=0,
                ymax=1,
                ylabel near ticks
            ]
            \addplot[color=red, width=2] coordinates {
                    (-4,0.009)
                    (0,0.009)
                    (0,1)
                    (4,1)
            };
        \end{axis}
    \end{tikzpicture}
\end{center}
Since the algebraic form of {$\sigma$} is a smoothed and not step function, it makes a huge difference in the way how weights and bias influence
the result of a sigmoid compared to a percpetron.
\newpage

\section{Current state}
Object detection is dealing with two major problems and therefore can be split into two processes which are object localization and object
classification.  \\
There are models which have a multi layer architecture, each performing complex tasks such as analyzing, localizing and classifying each
sections individually, making the cost high and the network in general slow but accurate.
Models that combine functionality in one layer, so that it for instance localizes and classifies at the same time, is making the
model smaller and therefore faster but less accurate.
The trade off between accuracy and speed is something that all networks deal with especially if the goal is to have a near real time object
detection system which is still reliable and accurate when doing so. \\
There are multiple algorithms covering various forms of this formula, called RCNN, F-RCNN, SSD, YOLO and so forth but first I will cover
the approach of detecting interesting regions in an image because this is the key problem in object detection.

\subsection{Object Localization \& Classification}
The segmentation of an image is the key approach to Object Localization and at the same time for Object Classification. Since images are always hierarchical the segmentation as well has
to be hierarchical. It has to be able to cover different criteria and forms, where maybe objects are part of a bigger total or just  smaller
objects which are inside one another. Since is not feasible to compute every possibility inside an image there has to be some kind of
separation through grids and scales. \\
As there are many approaches to algorithms as how to analyze and classify regions, I am going to cover three localization algorithms which
are key for object detection.

\subsubsection{Exhaustive Search}
Since an object can be located anywhere in an image the scope to look through is enormous. Making the computational cost for such a search very
expensive. To compensate for this cost most exhaustive search approaches such as the sliding window approach, constraint the size and aspect
ratio of the window over the grid, using weak classifiers and economic image features such as histogram of oriented gradients
(HOG).\cite{selectivesearch} \\ 
HOG approaches the problem of recognizing and detecting objects in an unknown image through corners. It analyzes the distribution and their
orientation, through which he is able to separate the image into multiple partitions or detect similar regions in different images.
Robert K. McConnell described the concept in a patent in 1986 but it became known in 2005 through a publication of Navneet Dalal and Bill
Triggs.\cite{hog}
\begin{center}
    \includegraphics[width=0.3\textwidth]{images/object_detection/Dlib_Learned-HOG-Detector.jpg} \cite{hog}
\end{center}

\subsubsection{Selective Search}
A selective search algorithm is subjected into 3 design ideas:\\
\begin{itemize}
    \item Capture All Scales \\ \\
        The idea is similar to the one of the Exhaustive Search that all objects have to be found. Therefore all scales have to be
        considered as a potential region. The best approach is a hierarchical grouping, where a initial sub segmentation has to
        be performed. \cite{selectivesearch}
        \begin{center}
            \includegraphics[width=0.5\textwidth]{images/object_detection/breakfast_fnh.jpg}\cite{sssegmentationc}
        \end{center}
        After the segmentation a grouping of regions is done by similarity. After each grouping a similarity between
        resulting regions and its neighbors are being calculated. This process is done until the whole image becomes a single region.\cite{selectivesearch}
        \begin{center}
            \includegraphics[width=0.5\textwidth]{images/object_detection/breakfast-top-200-proposals.jpg}\cite{sssegmentationbb}
        \end{center}
    \item Diversification \\ \\
        There is no single strategy for grouping regions together therefore multiple aspects have to be considered. Selective search
        diversifies by using a variety of color spaces with different invariance properties, by using different similarity measures and by
        varying the starting regions.  By using different color spaces, selective search is able to account for different scene and light
        conditions. To account for these variances selective search is performing the hierarchical grouping algorithm with different color
        spaces which have different invariance properties. \\
    \item Fast to Compute \\ \\
        Since each of these steps is a computation act, the main goal of this algorithm is to be fast and not create a bottleneck through its complexity.\cite{selectivesearch}
\end{itemize}
{S}\emph{color}: \\ \\
Measures color similarities for each region, where a color histogram of 25 bins is being created and normalized
with the L1 norm. Similarity is measured using the histogram intersection:\cite{selectivesearch} \\
\begin{center}
    \begin{equation*}
        S_{color}(r_i,r_j) = \sum_{n=1}^{n} min(c_i^{k},c_j^{k})
    \end{equation*}\cite{selectivesearch}
\end{center}
This histogram can be efficiently propagated through the hierarchy by: \\
\begin{center}
    \begin{equation*}
        \begin{split}
            C_t & = \frac{size(r_j)*C_i+size(r_j)*C_j}{size(r_i)+size(r_j)}
        \end{split}
    \end{equation*}\cite{selectivesearch}
\end{center}
{S}\emph{texture}: \\ \\
Selective Search measures texture similarity through the usage of the SIFT algorithm. SIFT stands for scale invariant
feature transform and is a algorithm to detect and describe features in a region. Further more it takes Gaussian derivatives
in eight orientations for each color channel, where a 10 bin histogram is being extracted. Similarity is measured
using the same histogram intersections and the propagation function is the same as for the color.\cite{selectivesearch}
\begin{center}
    \begin{equation*}
        S_{texture}(r_i,r_j) = \sum_{n=1}^{n} min(t_i^{k},t_j^{k})
    \end{equation*}\cite{selectivesearch}
\end{center}
{S}\emph{size}: \\ \\
In this step selective search encourages to merge small regions early. Doing so forces the algorithm to focus
on smaller regions which have not been merged yet first. This is encouraged because it hinders the algorithm from one
region gobbling up the rest of them one by one.\cite{selectivesearch}
\begin{center}
    \begin{equation*}
        \begin{split}
            S_{size}(r_i,r_j) & = 1 - \frac{size(r_i)+size(r_j)}{size(image)}
        \end{split}
    \end{equation*}\cite{selectivesearch}
\end{center}
{S}\emph{fill}: \\ \\
Measures how well region r$_i$ and r$_j$ fit into each other. The idea is to fill gaps if r{$_i$} is contained in
r$_j$, therefore it is only logical that these regions should be merged first in order to avoid holes. On the other hand
if r$_i$ and r$_j$ hardly touch each other it may form a strange region and therefore should not be merged. To keep
the process fast only the size of the regions and their contained bounding boxes are being used. Specifically the defined
BB$_{ij}$ has to be a tight bounding box around r$_i$ and r$_j$. Now S$_{fill}$ is the fraction of the image
contained in BB$_{ij}$ which is not by the regions of r$_i$ and r$_j$.\cite{selectivesearch}
\begin{center}
    \begin{equation*}
        \begin{split}
            fill(r_i, r_j) = 1 - \frac{size(BB_{ij}) - size(r_i) - size(r_j)}{size(image)}
        \end{split}
    \end{equation*}\cite{selectivesearch}
\end{center}
The final formula which measures the similarity is a combination of the above 4:
\begin{center}
    \begin{equation*}
        s(r_i,r_j) = a1s_{coulor}(r_i,r_j) + a2s_{texture}(r_i,r_j) + a3s_{size}(r_i,r_j) + a4s_{fill}(r_i,r_j)
    \end{equation*}\cite{selectivesearch}
\end{center}
\newpage
\subsubsection{Region Proposal Network in Faster R-CNN}
A Region Proposal Network (RPN) takes an image of any size as an input and outputs a set of rectangular object proposals, each with an
objectness score. After the initial proposals, a sliding window is being run over the feature map. The sliding
window is of size n x n. In this example it will be of size 3 x 3. So for each sliding window a set of 9 anchors are being generated, 3
scales and 3 aspect ratios.\cite{fasterrcnn}
\begin{center}
    \includegraphics[width=0.5\textwidth]{images/object_detection/rpn_anchors.png}
\end{center}
Furthermore for each sliding window anchor a value called Intersection of Union short IoU is being calculated. IoU represents the value of how
much the predicted bounding box overlaps with the ground truth. The ground truth is the basis for teaching the localization network and is a
set of data predefined by a human telling the network where, which objects are in the image. \\
The general idea of IoU is:\cite{fasterrcnn}
\begin{center}
    \begin{equation*}
        p =
        \begin{cases} 
            1 & \text{if IoU} > threshold \\
            -1 & \text{if IoU} < threshold \\
            0 & otherwise
        \end{cases}
    \end{equation*}
\end{center}
In the final step the results of the spatial sliding window is being fed into a smaller network which has two tasks. Classification (cls)
and regression (reg). The output of the regression represents a bounding box with (x, y, w, h). The classification value is a probability
indicating whether the bounding box contains a object or not.\cite{fasterrcnn}

\subsubsection{Single Shot Detector(SSD)}
Compared to the Faster R-CNN where the detection happens in two stages. First having the region proposal network generating proposals and
second then classifying them. SSD is a single feed forwards convolutional network to directly predict classes and anchor offsets
without requiring a second stage per-proposal.

\end{document}     
